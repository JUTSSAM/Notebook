---
title: "RBFN - 径向基函数网络"
date: 2018-12-27T21:33:59+08:00
draft: false
---

```
概念解释：
vector <=> patern 即 向量和模式的概念在此文中是相同的。
```
[TOC]

##  RBF( 径向基函数网络 )
- 将神经网络的设计看作是一个高维空间的曲线拟合（逼近）问题，采用不用的方法进行设计。
- 学习 => 在多维空间中寻找一个能够___最佳拟合(统计意义上的)___训练数据的曲面。
- 泛化 => 利用多维曲面对测试数据进行插值。（?多维空间中传统的严格插值法）

神经网络中的隐藏单元提供一个“函数”集，该函数集在输入模式（向量）拓展至**隐藏空间**时为其构造了一个任意的“基”；这个函数集中的函数就称为**径向基函数**。

最基本形式的径向基函数网络(RBFN)由三层构成：

- 输入层：源点（感知单元），网络与外界环境连接。
- 隐层：非线性变换
- 输出层：现行，为输入层的激活信号提供响应。

大多数情况下隐藏空间具有较高的维数，隐藏空间维度越高，逼近就越精确。

### 模式可分性的Cover定理

当使用RBF解决问题时，将 **复杂的模式分类问题** **非线性地** **投射** 到 **高维空间** 将比投射到 **低维空间** 更可能是 **线性可分的**。(根据Cover的论文)

具有线性可分性的分类问题更容易解决，通过RBF网络作为模式分类器更易得到线性可分。

给定一族曲面，每个曲面都能自然地将输入空间自然地分成两个区域。用 $ V$代表$N$个模式（向量）$\mathbf{x}_1...\mathbf{x}_N$的集合，每一个都分属于$\mathbf{V}_1$和$\mathbf{V}_2$中的一类。如果这族曲面中存在 **一个** 曲面能把分属于$\mathbf{V}_1$和$\mathbf{V}_2$的点分成两部分，则称这些点的二元划分关于这族曲面是 **可分** 的。

对于每一个模式向量$x∈V$，定义一个由一组实值函数$f$组成的向量$F$。假设模式$x$是$\mathbf{m}_0$维向量上的点，则函数将$\mathbf{m}_0$维输入空间的点映射到新的$\mathbf{m}_1$维空间的对应的点上。将$f$称为**隐藏函数**，它与前反馈神经网络中的隐藏单元起着相同的作用。生成的新的空间被称为**隐藏空间**或者**特征空间**。

当一个关于$V$的二分$\{\mathbf{V}_1,\mathbf{V}_2\}$是$F$可分的，如果存在m1维的向量w使得，

wfx > 0,x属于v1

wfx < 0,x属于v2

由wfx=0定义的超平面描述的F空间(也就是隐藏空间)中的分离曲面。？这个超平面的逆像，即定义输入空间中的分离曲面？

用r次模式向量坐标乘积的线性组合实现一个**自然类映射**。相对应的分离曲面被称为r阶有理簇。一个m0维空间的r阶有理簇，可描述为输入向量x的坐标的r次齐次方程，表示为



为了用齐次方程，将x0的值置为单位1。对于一个m0维的输入空间在式子中一共有 $(m0-r)!/m0!r!$ 个单项式。

该式可以描述分离曲面的的例子有超平面（一阶），二次曲面（二节），超球面（带有线性限制的二次曲面）等。通常情况下，线性可分性暗示着球面可分性，而球面可分性又暗示着二次可分性；然而反之不一定成立。（注：低维可分则高维可分，高维可分不一定低维可分）

在概率实验中，一个模式集合的可分性成为一个依赖于选择的二分以及输入空间中模式分布的随机事件。假设**激活模式**（输入向量）是根据空间中的概率特性独立随机选取的。同时假设所有的向量是等概率二分。令P表示某一随机选取的二分是f可分的概率，被选中的分离曲面有m1维自由度，根据Cover，

P(N,m1) = (1/2)^(N-1)*cigma(C(m,N-1))

相当于抛N-1次硬币有m1-1次或更少头像向上的概率

模式可分性的Cover定理包含两个基本方面：

- 由fi定义的隐藏函数的非线性构成
- 高维数的隐藏空间

如果在新的m1维空间中存在超平面使得fx可分，则该定义该超平面为隐藏空间（f空间）的**分离曲面**。

#### XOR问题##

四个二维输入点(1,1)，(0,1)，(1,0)，(0,0)。要求建立分类器产生二值输出响应。对应d={0,1}。

定义一对Gauss函数：

f1(x) = exp(-|x-t1|^2)

f2(x) = exp(-|x-t2|^2)

得到四个点，发现输入模式在映射到f1-f2平面上时，输入是线性可分的。将f1和f2作为perceptron模型的输入，则xor得到了解决。

- 相对于输入空间，隐藏空间的维度并没有增加，Gauss作为非线性隐藏，可以将xor问题转化为线性可分问题。

#### 曲面的分离能力

**推论**

- 一组随机指定的输入模式的集合在m1维空间中线性可分，它的元素数目的最大期望等于2m1。

？2m1是对一族具m1维自由度的决策曲面的分离能力的自然定义。？


### 插值问题
从关于模式可分性的Cover定理得到的重要思想是在解决一个非线性可分的模式分类问题时，如果将输入空间映射到一个新的位数足够高的空间去，将会有助于问题的解决。

可以用非线性变换将一个复杂的非线性**滤波**问题转化为一个较简单的线性**滤波**问题。

高维空间多变量插值理论。问题描述：

考虑一个由输入层、一个中间层和只有一个输出单元的输出层组成的前馈网路。实现从输入空间到隐藏空间的一个非线性映射，随后从隐藏空间到输出空间则是线性映射。令输入空间为m0维。则该网络相当于从m0维输入空间到1维输出空间的映射：

s:Rm0 -> R1

将s视为超平面，类似于s(x)=x*x可视为一条抛物线。通常情况下，曲面未知而且训练数据常常带有噪声。

训练阶段：曲面拟合过程的最优化，根据模式样本形式呈现给网络的已知数据

泛化阶段：在数据点之间插值

#### 高维空间多变量插值理论

给定一个包含N个不同点xi的集合和相应的N个实数的集合di，寻找函数F(RN->R1)满足插值条件F(xi)=di

插值曲面必须通过所有的训练数据点。

RBF就是要选择一个函数F具有形式：

F(x)=cigma(wi*f(|x-xi|))，其中f(|x-xi|)是函数集合，称为径向基函数。



##### Micchelli定理

保证插值矩阵是非奇异的。

- 如果X是R中N个不相同点的集合，则N×N阶插值矩阵是非奇异的。



满足micchelli定理的径向基函数：

1. 多二次函数

2. 逆多二次函数
3. Gauss函数

![1546394884628](/tmp/1546394884628.png)



### 作为不适定超曲面重建问题的监督学习

训练神经网络使其能够根据输入模式找到相应的输出模式，它的设计相当于学习一个超曲面使其能够根据输入确定输出。学习可以被视为给定一组可能稀疏的数据点的超曲面重建问题。

适定：假设有定义域X和值域Y以及固定但是未知的映射f

1. 存在性。每个输入变量都存在对应的输出
2. 唯一性。任何一对x1和x2，当且仅当x1=x2时，有f(x1)=f(x2)
3. 连续性。映射连续。

在现实场景中，问题往往是不适定的。



### 正则化理论

为了解决不适定问题。

基本思想：通过某些含有解的先验知识的非负的辅助泛函数来使解稳定。

![1546396410517](/tmp/1546396410517.png)![1546396588364](/tmp/1546396588364.png)

### 正则化网络

第一层m0个m0维节点，第二层是由直接和所有节点相连的非线性单元组成，一个隐藏单元对应一个数据点，每个隐藏单元由Green函数定义。输出层仅包含一个**线性**（线性加权和）单元，与所有隐藏单元相连。

![1546407548188](/tmp/1546407548188.png)

？？？Green 具有Gauss形式，则由该网络所得到的解在泛函F最小话的意义下将是一个最佳的内插解。？？

由逼近理论的观点，正则化网络具有如下三个期望的性质:

1. 正则化网络是一个通用逼近器，只要有足够的隐藏单元，就可以任意精度逼近定义在Rm0的紧子集上的任何多元连续函数
2. 因为逼近格式的未知系数是线性的，所以该网络具有最佳逼近性能。说明总存在一组系数拥有最佳逼近性能
3. 最佳的解指的是正则化网络使得测试训练样本表示的解与真实值偏差泛函的最小化。

### 广义的径向基函数

当输入向量维度太大时，计算出现困难，需要求一个正则化解的近似。Galerkin

F*(x)=cigma(wf(x))

m<N

### XOR问题

